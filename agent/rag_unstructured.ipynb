{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap # for print long text in wrapped lines\n",
    "from pprint import pprint # for print pretty json\n",
    "\n",
    "def format_text(text, width=80, indent=0):\n",
    "    # Split text into lines\n",
    "    lines = text.split('\\n')\n",
    "    # Format each line, keeping short lines unchanged\n",
    "    formatted_lines = []\n",
    "    for line in lines:\n",
    "        if len(line) <= width - indent:\n",
    "            # Keep short lines as is, just add indent\n",
    "            formatted_lines.append(\" \"*indent + line)\n",
    "        else:\n",
    "            # Wrap long lines\n",
    "            wrapped = textwrap.fill(line, width=width, initial_indent=\" \"*indent, subsequent_indent=\" \"*indent)\n",
    "            formatted_lines.append(wrapped)\n",
    "    return '\\n'.join(formatted_lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get embeddings and vector store, do semantic search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "# for convert text to embedding vectors, so that we can use it for similarity search and RAG\n",
    "from langchain_openai import OpenAIEmbeddings \n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "\n",
    "# from langchain_ollama import OllamaEmbeddings # alternatively use open source models on your own server\n",
    "# embeddings = OllamaEmbeddings(model=\"qwen2.5\")\n",
    "\n",
    "from langchain_unstructured import UnstructuredLoader # for load data from unstructured files \n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter # for split text into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to use unstructured, we need to set it up following: https://python.langchain.com/docs/integrations/document_loaders/unstructured_file/\n",
    "\n",
    "file_paths = [\n",
    "  \"../hardware/datasheets/TB6612FNG.pdf\"\n",
    "]\n",
    "loader = UnstructuredLoader(\n",
    "  file_paths,\n",
    "  chunking_strategy=\"basic\",\n",
    "  max_characters=1000000,\n",
    "  include_orig_elements=False,\n",
    "  )\n",
    "\n",
    "\n",
    "docs = loader.load()\n",
    "docs[0]\n",
    "\n",
    "pprint(docs[0].metadata)\n",
    "\n",
    "print(\"Number of LangChain documents:\", len(docs))\n",
    "print(\"Length of text in the document:\", len(docs[0].page_content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(format_text(docs[0].page_content[:200]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, chunk_overlap=200, add_start_index=True\n",
    ")\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "len(all_splits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(format_text(all_splits[2].page_content))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_1 = embeddings.embed_query(all_splits[0].page_content)\n",
    "vector_2 = embeddings.embed_query(all_splits[1].page_content)\n",
    "\n",
    "assert len(vector_1) == len(vector_2)\n",
    "print(f\"Generated vectors of length {len(vector_1)}\\n\")\n",
    "print(vector_1[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "\n",
    "vector_store = InMemoryVectorStore(embeddings)\n",
    "ids = vector_store.add_documents(documents=all_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# an example of similarity search fo a question\n",
    "results = vector_store.similarity_search(\n",
    "    \"What is the function of pin AO1?\"\n",
    ")\n",
    "\n",
    "print(\"Number of retrieved results:\", len(results))\n",
    "# print the first result\n",
    "print(format_text(results[0].page_content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(format_text(results[1].page_content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "# llm = ChatOpenAI(model=\"o1-preview\") # much stronger reasoning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the RAG with LangGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "from langchain import hub\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langgraph.graph import START, StateGraph\n",
    "from typing_extensions import List, TypedDict\n",
    "\n",
    "# Define prompt for question-answering\n",
    "prompt = hub.pull(\"rlm/rag-prompt\") # details in https://smith.langchain.com/hub/rlm/rag-prompt\n",
    "\n",
    "\n",
    "# Define state for application\n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    context: List[Document]\n",
    "    answer: str\n",
    "\n",
    "\n",
    "# Define application steps\n",
    "def retrieve(state: State):\n",
    "    retrieved_docs = vector_store.similarity_search(state[\"question\"])\n",
    "    return {\"context\": retrieved_docs}\n",
    "\n",
    "\n",
    "def generate(state: State):\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
    "    messages = prompt.invoke({\"question\": state[\"question\"], \"context\": docs_content})\n",
    "    response = llm.invoke(messages)\n",
    "    return {\"answer\": response.content}\n",
    "\n",
    "\n",
    "# Compile application and test\n",
    "graph_builder = StateGraph(State).add_sequence([retrieve, generate])\n",
    "graph_builder.add_edge(START, \"retrieve\")\n",
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = graph.invoke({\"question\": f\"Output the pin mapping of TB6612FNG in json format, including the pin name, number, and function.\"})\n",
    "\n",
    "# print(f'Context: {response[\"context\"]}\\n\\n')\n",
    "print(format_text(response[\"answer\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# an example of using the RAG for question answering\n",
    "\n",
    "response = graph.invoke({\"question\": \"What does this datasheet say about how to control the motor?\"})\n",
    "\n",
    "print(f'Context: {response[\"context\"]}\\n\\n')\n",
    "print(format_text(response[\"answer\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response[\"answer\"])\n",
    "summary = response[\"answer\"]\n",
    "# print(textwrap.fill(response[\"answer\"], width=80))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = graph.invoke({\"question\": f\"Give this code {summary}, for Arduino Nano, output the C code for the PWM driver, make sure the PWM driver has frequency of 5000 Hz.\"})\n",
    "\n",
    "# print(f'Context: {response[\"context\"]}\\n\\n')\n",
    "print(format_text(response[\"answer\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_code = response[\"answer\"]\n",
    "response = graph.invoke({\"question\": f\"Give original code {sample_code}, for Arduino Nano, justify values you used to make sure the PWM driver has frequency of 5000 Hz.\"})\n",
    "\n",
    "# print(f'Context: {response[\"context\"]}\\n\\n')\n",
    "print(format_text(response[\"answer\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for step in graph.stream(\n",
    "    {\"question\": \"Output the pin mapping of TB6612FNG in json format, including the pin name, number, and function.\"}, stream_mode=\"updates\"\n",
    "):\n",
    "    print(f\"{step}\\n\\n----------------\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
